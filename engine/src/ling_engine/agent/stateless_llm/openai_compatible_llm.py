"""Description: This file contains the implementation of the `AsyncLLM` class.
This class is responsible for handling asynchronous interaction with OpenAI API compatible
endpoints for language generation.
"""

from typing import AsyncIterator, List, Dict, Any
import time
from openai import (
    AsyncStream,
    AsyncOpenAI,
    APIError,
    APIConnectionError,
    RateLimitError,
)
from openai.types.chat import ChatCompletionChunk
from loguru import logger

from .stateless_llm_interface import StatelessLLMInterface
from ...utils.token_counter import token_stats, TokenCalculator, TokenUsage

class AsyncLLM(StatelessLLMInterface):
    def __init__(
        self,
        model: str,
        base_url: str,
        llm_api_key: str = "z",
        organization_id: str = "z",
        project_id: str = "z",
        temperature: float = 1.0,
        tier: int = None,
    ):
        """
        Initializes an instance of the `AsyncLLM` class.

        Parameters:
        - model (str): The model to be used for language generation.
        - base_url (str): The base URL for the OpenAI API.
        - organization_id (str, optional): The organization ID for the OpenAI API. Defaults to "z".
        - project_id (str, optional): The project ID for the OpenAI API. Defaults to "z".
        - llm_api_key (str, optional): The API key for the OpenAI API. Defaults to "z".
        - temperature (float, optional): What sampling temperature to use, between 0 and 2. Defaults to 1.0.
        - tier (int, optional): Priority tier for OpenAI API (1-5, higher number = higher priority). Defaults to None.
        """
        self.base_url = base_url
        self.model = model
        self.temperature = temperature
        self.tier = tier
        # åªæœ‰åœ¨éé»˜è®¤å€¼æ—¶æ‰ä¼ é€’ organization å’Œ project
        client_kwargs = {
            "base_url": base_url,
            "api_key": llm_api_key,
        }

        # åªæœ‰åœ¨æœ‰æ•ˆå€¼æ—¶æ‰æ·»åŠ  organization å’Œ project å‚æ•°
        if organization_id and organization_id != "z":
            client_kwargs["organization"] = organization_id
        if project_id and project_id != "z":
            client_kwargs["project"] = project_id

        self.client = AsyncOpenAI(**client_kwargs)

        logger.info(
            f"Initialized AsyncLLM with the parameters: {self.base_url}, {self.model}"
        )
        
        # æ˜¾ç¤º tier é…ç½®ä¿¡æ¯
        if self.tier is not None:
            if self.base_url and "api.openai.com" in self.base_url:
                logger.info(f"ğŸš€ OpenAI Priority tier configured: {self.tier} (will be used for requests)")
            else:
                logger.info(f"âš ï¸ Priority tier configured ({self.tier}) but not applicable for non-OpenAI endpoint: {self.base_url}")
        else:
            logger.info("â„¹ï¸ No priority tier configured (using default priority)")

    async def chat_completion_with_tools(
        self, messages: List[Dict[str, Any]], tools: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        ä¸“é—¨ç”¨äºå¤„ç†å¸¦å·¥å…·çš„èŠå¤©å®Œæˆï¼ˆéæµå¼ï¼‰

        Returns:
            Dict containing:
            - content: str - å“åº”æ–‡æœ¬
            - tool_calls: List[Dict] - å·¥å…·è°ƒç”¨åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
        """
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages_with_system = messages

            # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆéæµå¼ï¼‰
            request_params = {
                "messages": messages_with_system,
                "model": self.model,
                "stream": False,  # ä¸ä½¿ç”¨æµå¼
                "temperature": self.temperature,
                "tools": tools,
                "tool_choice": "auto"
            }

            logger.info(f"Calling LLM with {len(tools)} tools (non-streaming)")

            # å‘é€è¯·æ±‚
            response = await self.client.chat.completions.create(**request_params)

            # å¤„ç†å“åº”
            choice = response.choices[0]
            result = {
                "content": choice.message.content or "",
                "tool_calls": []
            }

            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨
            if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                for tool_call in choice.message.tool_calls:
                    result["tool_calls"].append({
                        "id": tool_call.id,
                        "type": "function",
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    })
                logger.info(f"LLM returned {len(result['tool_calls'])} tool calls")

            return result

        except Exception as e:
            logger.error(f"Error in chat_completion_with_tools: {e}")
            return {"content": f"Error: {str(e)}", "tool_calls": []}

    async def chat_completion(
        self, messages: List[Dict[str, Any]], system: str = None, tools: List[Dict[str, Any]] = None
    ) -> AsyncIterator[str]:
        """
        Generates a chat completion using the OpenAI API asynchronously.

        Parameters:
        - messages (List[Dict[str, Any]]): The list of messages to send to the API.
        - system (str, optional): System prompt to use for this completion.

        Yields:
        - str: The content of each chunk from the API response.

        Raises:
        - APIConnectionError: When the server cannot be reached
        - RateLimitError: When a 429 status code is received
        - APIError: For other API-related errors
        """
        logger.debug(f"Starting chat completion with model: {self.model}")
        logger.debug(f"Base URL: {self.base_url}")
        logger.debug(f"Temperature: {self.temperature}")
        logger.debug(f"System prompt: {system}")
        logger.debug(f"Messages: {messages}")
        
        stream = None
        full_response = ""
        input_tokens = 0
        try:
            # If system prompt is provided, add it to the messages
            messages_with_system = messages
            if system:
                messages_with_system = [
                    {"role": "system", "content": system},
                    *messages,
                ]
                logger.debug(f"Added system prompt. Total messages: {len(messages_with_system)}")

            # è®¡ç®—è¾“å…¥token
            calculator = TokenCalculator(self.model)
            input_tokens = calculator.count_messages_tokens(messages_with_system).prompt_tokens
            logger.info(f"[Tokenè·Ÿè¸ª] å¯¹è¯å¼€å§‹ - æ¨¡å‹: {self.model}, è¾“å…¥Token: {input_tokens}")

            logger.info("Sending request to LLM API...")
            
            # è®°å½•è¯·æ±‚å¼€å§‹æ—¶é—´
            request_start_time = time.time()
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_params = {
                "messages": messages_with_system,
                "model": self.model,
                "stream": True,
                "temperature": self.temperature,
            }

            # å¦‚æœæä¾›äº† toolsï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if tools:
                request_params["tools"] = tools
                request_params["tool_choice"] = "auto"  # è®© LLM è‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·
                logger.info(f"Added {len(tools)} tools to request")
            
            # å¤„ç† OpenAI Priority tier
            if self.tier is not None and self.base_url and "api.openai.com" in self.base_url:
                # OpenAI service_tier ç­‰çº§æ˜ å°„:
                # tier 5: priority (æœ€é«˜ä¼˜å…ˆçº§)
                # tier 4: priority (é«˜ä¼˜å…ˆçº§) 
                # tier 1-3: default (é»˜è®¤)
                if self.tier >= 4:  
                    request_params["service_tier"] = "priority"
                    priority_level = "æœ€é«˜ä¼˜å…ˆçº§" if self.tier == 5 else "é«˜ä¼˜å…ˆçº§"
                    logger.info(f"ğŸš€ ä½¿ç”¨ OpenAI Priority æœåŠ¡ - ç­‰çº§ {self.tier} ({priority_level})")
                else:
                    request_params["service_tier"] = "default" 
                    logger.info(f"ğŸ“Š ä½¿ç”¨ OpenAI é»˜è®¤æœåŠ¡ - ç­‰çº§ {self.tier}")
                    
                # æ·»åŠ æ€§èƒ½æç¤º
                logger.info("ğŸ’¡ Priority æ•ˆæœåœ¨é«˜å¹¶å‘æ—¶æœŸæœ€æ˜æ˜¾ï¼Œå½“å‰å¯èƒ½å› è´Ÿè½½è¾ƒä½è€Œå·®å¼‚ä¸å¤§")
                
            stream: AsyncStream[
                ChatCompletionChunk
            ] = await self.client.chat.completions.create(**request_params)
            logger.info("Successfully connected to LLM API")
            
            first_response_logged = False
            async for chunk in stream:
                if chunk.choices[0].delta.content is None:
                    chunk.choices[0].delta.content = ""
                
                # è®°å½•ç¬¬ä¸€æ¬¡æ”¶åˆ°ä»»ä½•chunkçš„æ—¶é—´ï¼ˆæ— è®ºå†…å®¹æ˜¯å¦ä¸ºç©ºï¼‰
                if not first_response_logged:
                    first_response_time = time.time()
                    response_latency = (first_response_time - request_start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    logger.info(f"â±ï¸ LLMé¦–æ¬¡å“åº”æ—¶é—´: {response_latency:.0f}ms (æ¨¡å‹: {self.model})")
                    print(f"[DEBUG] â±ï¸ LLMé¦–æ¬¡å“åº”æ—¶é—´: {response_latency:.0f}ms (æ¨¡å‹: {self.model})")  # é¢å¤–çš„è°ƒè¯•è¾“å‡º
                    first_response_logged = True
                    
                full_response += chunk.choices[0].delta.content
                yield chunk.choices[0].delta.content

        except APIConnectionError as e:
            error_msg = (
                f"Error calling the chat endpoint: Connection error. Failed to connect to the LLM API.\n"
                f"Base URL: {self.base_url}\n"
                f"Model: {self.model}\n"
                f"Error details: {str(e.__cause__)}\n"
                f"Check the configurations and the reachability of the LLM backend."
            )
            logger.error(error_msg)
            logger.error(f"Full error: {e}")
            logger.error(f"Error cause: {e.__cause__}")
            yield "Error calling the chat endpoint: Connection error. Failed to connect to the LLM API. Check the configurations and the reachability of the LLM backend. See the logs for details. Troubleshooting with documentation: [https://ling-engine.github.io/docs/faq#%E9%81%87%E5%88%B0-error-calling-the-chat-endpoint-%E9%94%99%E8%AF%AF%E6%80%8E%E4%B9%88%E5%8A%9E]"

        except RateLimitError as e:
            error_msg = (
                f"Error calling the chat endpoint: Rate limit exceeded\n"
                f"Response: {e.response}\n"
                f"Base URL: {self.base_url}\n"
                f"Model: {self.model}"
            )
            logger.error(error_msg)
            logger.error(f"Full error: {e}")
            yield "Error calling the chat endpoint: Rate limit exceeded. Please try again later. See the logs for details."

        except APIError as e:
            error_msg = (
                f"LLM API Error occurred:\n"
                f"Error type: {type(e).__name__}\n"
                f"Error message: {str(e)}\n"
                f"Base URL: {self.base_url}\n"
                f"Model: {self.model}\n"
                f"Temperature: {self.temperature}\n"
                f"Number of messages: {len(messages)}"
            )
            logger.error(error_msg)
            logger.error(f"Full error: {e}")
            if hasattr(e, 'response'):
                logger.error(f"Response: {e.response}")
            yield "Error calling the chat endpoint: Error occurred while generating response. See the logs for details."

        except Exception as e:
            error_msg = (
                f"Unexpected error in chat completion:\n"
                f"Error type: {type(e).__name__}\n"
                f"Error message: {str(e)}\n"
                f"Base URL: {self.base_url}\n"
                f"Model: {self.model}"
            )
            logger.error(error_msg)
            logger.error(f"Full error: {e}")
            yield "Error calling the chat endpoint: Unexpected error occurred. See the logs for details."

        finally:
            # è®¡ç®—è¾“å‡ºtokenå’Œæˆæœ¬
            if stream:
                completion_tokens = calculator.count_tokens(full_response) if full_response else 0
                total_tokens = input_tokens + completion_tokens
                
                usage = TokenUsage(
                    prompt_tokens=input_tokens,
                    completion_tokens=completion_tokens,
                    total_tokens=total_tokens
                )
                
                # ä¼°ç®—æˆæœ¬
                cost_info = calculator.estimate_cost(usage)
                
                # è®°å½•tokenä½¿ç”¨æƒ…å†µ
                token_stats.add_usage(
                    model=self.model,
                    usage=usage,
                    cost=cost_info.total_cost,
                    metadata={
                        "service_type": "llm",
                        "model": self.model,
                        "messages_count": len(messages_with_system) if 'messages_with_system' in locals() else len(messages)
                    }
                )
                
                logger.info(f"[Tokenè·Ÿè¸ª] å¯¹è¯ç»“æŸ - æ¨¡å‹: {self.model}, è¾“å‡ºToken: {completion_tokens}, " +
                            f"æ€»Token: {total_tokens}, æˆæœ¬: ${cost_info.total_cost:.6f}")
                
                logger.debug("Chat completion finished.")
                await stream.close()
                logger.debug("Stream closed.")