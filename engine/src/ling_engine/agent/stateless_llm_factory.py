from typing import Type, Optional

from loguru import logger

from .stateless_llm.stateless_llm_interface import StatelessLLMInterface
from .stateless_llm.openai_compatible_llm import AsyncLLM as OpenAICompatibleLLM
from .stateless_llm.ollama_llm import OllamaLLM
from .stateless_llm.claude_llm import AsyncLLM as ClaudeLLM
from ..config_manager import Config


def create_llm(config: Config) -> Optional[StatelessLLMInterface]:
    """Create LLM based on configuration

    Args:
        config: Configuration object

    Returns:
        LLM instance or None if creation fails
    """
    try:
        logger.info("ğŸ” åˆ†æé…ç½®ç»“æ„...")

        if not config:
            logger.error("âŒ Config å¯¹è±¡ä¸º None")
            return None

        if not config.character_config:
            logger.error("âŒ character_config ä¸º None")
            return None

        if not config.character_config.agent_config:
            logger.error("âŒ agent_config ä¸º None")
            return None

        agent_config = config.character_config.agent_config
        llm_provider = agent_config.conversation_agent_choice
        logger.info(f"ğŸ” æ£€æµ‹åˆ°å¯¹è¯ä»£ç†é€‰æ‹©: {llm_provider}")

        # Get LLM config based on provider
        if llm_provider == "basic_memory_agent":
            if not agent_config.agent_settings:
                logger.error("âŒ agent_settings ä¸º None")
                return None
            if not agent_config.agent_settings.basic_memory_agent:
                logger.error("âŒ basic_memory_agent è®¾ç½®ä¸º None")
                return None

            basic_memory_settings = agent_config.agent_settings.basic_memory_agent
            llm_provider = basic_memory_settings.llm_provider
            logger.info(f"ğŸ” ä» basic_memory_agent è®¾ç½®ä¸­è·å– LLM æä¾›å•†: {llm_provider}")

        if not llm_provider:
            logger.error("âŒ LLM provider not specified")
            logger.error("âŒ è¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® LLM æä¾›å•†")
            return None

        # Get LLM config
        if not agent_config.llm_configs:
            logger.error("âŒ llm_configs ä¸º None")
            return None

        llm_config = agent_config.llm_configs.dict().get(llm_provider, {})
        if not llm_config:
            logger.error(f"âŒ æ‰¾ä¸åˆ° LLM æä¾›å•†çš„é…ç½®: {llm_provider}")
            available_configs = list(agent_config.llm_configs.dict().keys())
            logger.error(f"âŒ å¯ç”¨çš„é…ç½®: {available_configs}")
            return None

        logger.info(f"âœ… æ‰¾åˆ° LLM é…ç½®: {llm_provider}")
        logger.debug(f"ğŸ” LLM é…ç½®å†…å®¹: {llm_config}")

        return LLMFactory.create_llm(
            llm_provider=llm_provider,
            **llm_config
        )
    except Exception as e:
        logger.error(f"âŒ Failed to create LLM: {e}", exc_info=True)
        return None

class LLMFactory:
    @staticmethod
    def create_llm(llm_provider: str, **kwargs) -> Type[StatelessLLMInterface]:
        """Create an LLM based on the configuration.

        Args:
            llm_provider: The type of LLM to create
            **kwargs: Additional arguments
        """
        logger.info(f"ğŸ”§ æ­£åœ¨åˆå§‹åŒ– LLM: {llm_provider}")
        logger.debug(f"ğŸ” LLM å‚æ•°: {kwargs}")

        try:
            if (
                llm_provider == "openai_compatible_llm"
                or llm_provider == "openai_llm"
                or llm_provider == "gemini_llm"
                or llm_provider == "zhipu_llm"
                or llm_provider == "deepseek_llm"
                or llm_provider == "groq_llm"
                or llm_provider == "mistral_llm"
            ):
                # æ£€æŸ¥å¿…éœ€çš„å‚æ•°
                required_params = ["model", "llm_api_key"]
                missing_params = [param for param in required_params if not kwargs.get(param)]
                if missing_params:
                    logger.error(f"âŒ OpenAI å…¼å®¹ LLM ç¼ºå°‘å¿…éœ€å‚æ•°: {missing_params}")
                    return None

                logger.info(f"âœ… åˆ›å»º OpenAI å…¼å®¹ LLM: {kwargs.get('model')}")
                return OpenAICompatibleLLM(
                    model=kwargs.get("model"),
                    base_url=kwargs.get("base_url"),
                    llm_api_key=kwargs.get("llm_api_key"),
                    organization_id=kwargs.get("organization_id"),
                    project_id=kwargs.get("project_id"),
                    temperature=kwargs.get("temperature", 1.0),
                    tier=kwargs.get("tier"),
                )

            elif llm_provider == "ollama_llm":
                logger.info(f"âœ… åˆ›å»º Ollama LLM: {kwargs.get('model')}")
                return OllamaLLM(
                    model=kwargs.get("model"),
                    base_url=kwargs.get("base_url"),
                    llm_api_key=kwargs.get("llm_api_key"),
                    organization_id=kwargs.get("organization_id"),
                    project_id=kwargs.get("project_id"),
                    temperature=kwargs.get("temperature"),
                    keep_alive=kwargs.get("keep_alive"),
                    unload_at_exit=kwargs.get("unload_at_exit"),
                )

            elif llm_provider == "llama_cpp_llm":
                from .stateless_llm.llama_cpp_llm import LLM as LlamaLLM

                logger.info(f"âœ… åˆ›å»º Llama.cpp LLM: {kwargs.get('model_path')}")
                return LlamaLLM(
                    model_path=kwargs.get("model_path"),
                )

            elif llm_provider == "claude_llm":
                logger.info(f"âœ… åˆ›å»º Claude LLM: {kwargs.get('model')}")
                return ClaudeLLM(
                    system=kwargs.get("system_prompt"),
                    base_url=kwargs.get("base_url"),
                    model=kwargs.get("model"),
                    llm_api_key=kwargs.get("llm_api_key"),
                )

            elif llm_provider == "anthropic_llm":
                # Anthropic native â€” åˆ›å»ºä¸€ä¸ª OpenAI å…¼å®¹çš„è–„åŒ…è£…
                # å®é™…çš„ ChatAnthropic åœ¨ LangchainAgentWrapper ä¸­åˆ›å»º
                logger.info(f"âœ… åˆ›å»º Anthropic LLM (native): {kwargs.get('model')}")
                return OpenAICompatibleLLM(
                    model=kwargs.get("model", "claude-sonnet-4-20250514"),
                    base_url=kwargs.get("base_url", "https://api.anthropic.com/v1"),
                    llm_api_key=kwargs.get("llm_api_key"),
                    temperature=kwargs.get("temperature", 0.7),
                )

            else:
                logger.error(f"âŒ ä¸æ”¯æŒçš„ LLM æä¾›å•†: {llm_provider}")
                supported_providers = [
                    "openai_compatible_llm", "openai_llm", "gemini_llm",
                    "zhipu_llm", "deepseek_llm", "groq_llm", "mistral_llm",
                    "ollama_llm", "llama_cpp_llm", "claude_llm", "anthropic_llm"
                ]
                logger.error(f"âŒ æ”¯æŒçš„æä¾›å•†: {supported_providers}")
                return None

        except Exception as e:
            logger.error(f"âŒ LLM åˆ›å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            return None

__all__ = ['LLMFactory', 'create_llm']
