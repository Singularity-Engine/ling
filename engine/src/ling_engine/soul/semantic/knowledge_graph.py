"""
çŸ¥è¯†å›¾è°± â€” MongoDB èŠ‚ç‚¹+è¾¹+$graphLookup æŸ¥è¯¢

è®¾è®¡:
- è¾¹æ–‡æ¡£åè§„èŒƒåŒ–: source_label/target_label ç›´æ¥å­˜åœ¨è¾¹ä¸Šï¼Œé¿å… $graphLookup è·¨é›†åˆ join
- èŠ‚ç‚¹å”¯ä¸€é”®: (user_id, label) â€” åŒä¸€ç”¨æˆ·åŒåæ¦‚å¿µå»é‡
- upsert è¯­ä¹‰: é‡å¤èŠ‚ç‚¹ â†’ mention_count++, last_confirmed æ›´æ–°
- confidence è¯»æ—¶è®¡ç®— (ä¸å­˜å‚¨) â€” åŸºäº mention_count + æ—¶é—´è¡°å‡
"""

import asyncio
import math
import re
from datetime import datetime, timezone
from hashlib import md5
from typing import Dict, List, Optional

from loguru import logger

# ğŸ§¬: confidence æ—¶é—´è¡°å‡å¸¸é‡
CONFIDENCE_HALF_LIFE_DAYS = 180  # 6 ä¸ªæœˆæœªæåŠ â†’ confidence å‡åŠ

_kg_instance: Optional["KnowledgeGraph"] = None


def get_knowledge_graph() -> "KnowledgeGraph":
    """å•ä¾‹å·¥å‚"""
    global _kg_instance
    if _kg_instance is None:
        _kg_instance = KnowledgeGraph()
    return _kg_instance


def _calc_confidence(mention_count: int, last_confirmed: datetime) -> float:
    """åŠ¨æ€è®¡ç®—èŠ‚ç‚¹ç½®ä¿¡åº¦ â€” åŸºäºæåŠæ¬¡æ•° + æ—¶é—´è¡°å‡

    è¯»æ—¶è®¡ç®—ï¼Œä¸å­˜å‚¨åˆ°æ–‡æ¡£ã€‚
    """
    days = (datetime.now(timezone.utc) - last_confirmed).days
    time_factor = 0.5 ** (days / CONFIDENCE_HALF_LIFE_DAYS)
    return min(1.0, 0.3 + 0.1 * math.log(max(mention_count, 1))) * time_factor


class KnowledgeGraph:
    """MongoDB çŸ¥è¯†å›¾è°± â€” èŠ‚ç‚¹+è¾¹+$graphLookup æŸ¥è¯¢"""

    async def _get_nodes_coll(self):
        from ..storage.soul_collections import get_collection, SEMANTIC_NODES
        return await get_collection(SEMANTIC_NODES)

    async def _get_edges_coll(self):
        from ..storage.soul_collections import get_collection, SEMANTIC_EDGES
        return await get_collection(SEMANTIC_EDGES)

    async def upsert_node(
        self, user_id: str, label: str, category: str,
        properties: Dict = None,
    ) -> Optional[str]:
        """upsert èŠ‚ç‚¹ â€” åŸå­æ“ä½œ, æ— ç«æ€

        ä½¿ç”¨ update_one(upsert=True) æ›¿ä»£ find+insertã€‚
        label æˆªæ–­åˆ° 50 å­—ç¬¦ + æ§åˆ¶å­—ç¬¦è¿‡æ»¤ã€‚
        ä¸å­˜å‚¨ confidence (è¯»æ—¶è®¡ç®—)ã€‚
        """
        # ğŸ”: æ ‡ç­¾æ¸…ç†
        label = re.sub(r'[\x00-\x1f\x7f]', '', label.strip())[:50]
        if not label:
            return None

        coll = await self._get_nodes_coll()
        if coll is None:
            return None

        node_id = f"{user_id}:{md5(label.encode()).hexdigest()[:8]}"
        now = datetime.now(timezone.utc)

        try:
            update_doc = {
                "$inc": {"mention_count": 1},
                "$set": {
                    "last_confirmed": now,
                    "category": category,
                },
                "$setOnInsert": {
                    "node_id": node_id,
                    "first_learned": now,
                    "user_confirmed": False,
                },
            }
            if properties:
                update_doc["$set"]["properties"] = properties

            await coll.update_one(
                {"user_id": user_id, "label": label},
                update_doc,
                upsert=True,
            )
            return node_id
        except Exception as e:
            logger.debug(f"[Soul] KG upsert_node failed: {e}")
            return None

    async def _cascade_label_update(
        self, user_id: str, old_label: str, new_label: str,
    ):
        """label å˜æ›´æ—¶çº§è”æ›´æ–° edges ä¸­çš„ source_label/target_label"""
        coll = await self._get_edges_coll()
        if coll is None:
            return
        try:
            await coll.update_many(
                {"user_id": user_id, "source_label": old_label},
                {"$set": {"source_label": new_label}},
            )
            await coll.update_many(
                {"user_id": user_id, "target_label": old_label},
                {"$set": {"target_label": new_label}},
            )
        except Exception as e:
            logger.debug(f"[Soul] KG cascade label update failed: {e}")

    async def upsert_edge(
        self, user_id: str, source_label: str, target_label: str,
        relation: str, strength: float = 0.5,
    ) -> Optional[str]:
        """upsert è¾¹ â€” åŸå­æ“ä½œ, $max ä¿ç•™æœ€é«˜ strength

        è¾¹æ–‡æ¡£åŒæ—¶å­˜ 4 å­—æ®µ: source_id, target_id, source_label, target_labelã€‚
        source_label/target_label ç”¨äº $graphLookup; source_id/target_id ç”¨äºçº§è”åˆ é™¤ã€‚
        """
        coll = await self._get_edges_coll()
        if coll is None:
            return None

        source_id = f"{user_id}:{md5(source_label.encode()).hexdigest()[:8]}"
        target_id = f"{user_id}:{md5(target_label.encode()).hexdigest()[:8]}"
        now = datetime.now(timezone.utc)

        try:
            await coll.update_one(
                {
                    "user_id": user_id,
                    "source_label": source_label,
                    "target_label": target_label,
                    "relation": relation,
                },
                {
                    "$max": {"strength": strength},
                    "$set": {"last_updated": now},
                    "$setOnInsert": {
                        "source_id": source_id,
                        "target_id": target_id,
                        "created_at": now,
                    },
                },
                upsert=True,
            )
            return f"{source_id}->{target_id}"
        except Exception as e:
            logger.debug(f"[Soul] KG upsert_edge failed: {e}")
            return None

    async def trace_context(
        self, user_id: str, start_label: str,
        max_depth: int = 2, limit: int = 5,
    ) -> List[str]:
        """$graphLookup ä»èµ·å§‹èŠ‚ç‚¹è¿½è¸ªå…³ç³»é“¾

        åªè¿”å› strength > 0.6 çš„è¾¹é“¾è·¯ (ç½®ä¿¡åº¦é—¨æ§›)ã€‚
        å¯¹æ¶‰åŠçš„èŠ‚ç‚¹åš confidence è¿‡æ»¤ (ä½ confidence èŠ‚ç‚¹çš„é“¾è·¯ä¸è¾“å‡º)ã€‚
        è¾“å‡ºåŠ ä¸ç¡®å®šæªè¾ã€‚è¿œæœŸé“¾è·¯ (>90å¤©) æè¿°æ›´æ¨¡ç³Šã€‚
        timeout: 200ms
        """
        coll = await self._get_edges_coll()
        if coll is None:
            return []

        try:
            from ..storage.soul_collections import SEMANTIC_EDGES
            pipeline = [
                {"$match": {"user_id": user_id, "source_label": start_label}},
                {"$graphLookup": {
                    "from": SEMANTIC_EDGES,
                    "startWith": "$target_label",
                    "connectFromField": "target_label",
                    "connectToField": "source_label",
                    "as": "chain",
                    "maxDepth": max_depth - 1,
                    "depthField": "depth",
                    "restrictSearchWithMatch": {
                        "user_id": user_id,
                        "strength": {"$gt": 0.6},
                    },
                }},
                {"$project": {
                    "source_label": 1, "target_label": 1, "relation": 1,
                    "strength": 1, "last_updated": 1,
                    "chain.source_label": 1, "chain.target_label": 1,
                    "chain.relation": 1, "chain.depth": 1,
                    "chain.last_updated": 1,
                }},
                {"$limit": limit},
            ]

            # é¢„åŠ è½½æ¶‰åŠèŠ‚ç‚¹çš„ confidence (æ‰¹é‡æŸ¥è¯¢ï¼Œä¸é€ä¸ª)
            node_confidence = await self._batch_node_confidence(user_id, start_label)

            results = []
            async for doc in coll.aggregate(pipeline):
                # ç›´æ¥è¾¹
                src = doc.get("source_label", "")
                tgt = doc.get("target_label", "")
                rel = doc.get("relation", "related")
                last_upd = doc.get("last_updated")

                # confidence è¿‡æ»¤: æ¶‰åŠçš„èŠ‚ç‚¹ confidence < 0.3 åˆ™è·³è¿‡
                if self._low_confidence(src, node_confidence) or \
                   self._low_confidence(tgt, node_confidence):
                    continue

                desc = self._format_trace(src, tgt, rel, last_upd)
                if desc:
                    results.append(desc)

                # é“¾è·¯è¾¹
                for chain_item in doc.get("chain", []):
                    c_src = chain_item.get("source_label", "")
                    c_tgt = chain_item.get("target_label", "")
                    c_rel = chain_item.get("relation", "related")
                    c_upd = chain_item.get("last_updated")
                    if self._low_confidence(c_src, node_confidence) or \
                       self._low_confidence(c_tgt, node_confidence):
                        continue
                    desc = self._format_trace(c_src, c_tgt, c_rel, c_upd)
                    if desc:
                        results.append(desc)

            return results[:limit]
        except Exception as e:
            logger.debug(f"[Soul] KG trace_context failed: {e}")
            return []

    async def _batch_node_confidence(
        self, user_id: str, start_label: str,
    ) -> Dict[str, float]:
        """æ‰¹é‡æŸ¥è¯¢èµ·å§‹æ ‡ç­¾ç›¸å…³èŠ‚ç‚¹çš„ confidence (è¯»æ—¶è®¡ç®—)"""
        nodes_coll = await self._get_nodes_coll()
        if nodes_coll is None:
            return {}
        try:
            cursor = nodes_coll.find(
                {"user_id": user_id},
                projection={"label": 1, "mention_count": 1, "last_confirmed": 1, "_id": 0},
                limit=50,
            )
            conf_map = {}
            async for doc in cursor:
                label = doc.get("label", "")
                mc = doc.get("mention_count", 1)
                lc = doc.get("last_confirmed")
                if label and lc:
                    conf_map[label] = _calc_confidence(mc, lc)
            return conf_map
        except Exception:
            return {}

    @staticmethod
    def _low_confidence(label: str, conf_map: Dict[str, float]) -> bool:
        """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦ confidence è¿‡ä½ (< 0.3 è§†ä¸ºä¸å¯é )"""
        if not label or label not in conf_map:
            return False  # æœªçŸ¥èŠ‚ç‚¹ä¸è¿‡æ»¤ (å¯èƒ½æ˜¯è¾¹ä¸Šçš„ label æœªæ”¶å½•ä¸ºèŠ‚ç‚¹)
        return conf_map[label] < 0.3

    @staticmethod
    def _format_trace(
        source: str, target: str, relation: str,
        last_updated: Optional[datetime] = None,
    ) -> Optional[str]:
        """æ ¼å¼åŒ–è¿½è¸ªç»“æœ â€” åŠ ä¸ç¡®å®šæªè¾, è¿œæœŸæ›´æ¨¡ç³Š"""
        if not source or not target:
            return None

        # ğŸ§¬: è¿œæœŸé“¾è·¯ (>90å¤©) æè¿°æ›´æ¨¡ç³Š
        days_ago = 0
        if last_updated:
            try:
                if isinstance(last_updated, datetime):
                    days_ago = (datetime.now(timezone.utc) - last_updated).days
            except Exception:
                pass

        rel_map = {
            "cause": "å¯èƒ½å¯¼è‡´äº†",
            "goal": "çš„ç›®æ ‡å¯èƒ½æ˜¯",
            "method": "å¯èƒ½é€šè¿‡",
            "context": "å¯èƒ½ä¸â€¦æœ‰å…³è”",
            "part_of": "å¯èƒ½æ˜¯â€¦çš„ä¸€éƒ¨åˆ†",
            "conflict": "å¯èƒ½ä¸â€¦å­˜åœ¨çŸ›ç›¾",
            "leads_to": "å¯èƒ½ä¼šå¼•å‘",
        }
        rel_text = rel_map.get(relation, "å¯èƒ½ä¸â€¦æœ‰å…³")

        if days_ago > 90:
            return f"å¥½åƒæåˆ°è¿‡{source}å’Œ{target}ä¹‹é—´æœ‰æŸç§è”ç³»"
        else:
            return f"{source}{rel_text}{target}"

    async def find_matching_labels(
        self, user_id: str, query: str, limit: int = 2,
    ) -> List[str]:
        """ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–æ¦‚å¿µè¯ï¼Œç”¨ $or + $regex åŒ¹é…çŸ¥è¯†å›¾è°±èŠ‚ç‚¹

        åˆ†è¯ç­–ç•¥: æå–è‹±æ–‡å•è¯(å« C++/.NET ç­‰) + ä¸­æ–‡ 2-4 å­—è¯ç»„ã€‚
        æŒ‰ mention_count é™åºæ’åºï¼Œé¢‘ç¹æåŠçš„æ¦‚å¿µä¼˜å…ˆåŒ¹é…ã€‚
        ç”¨ re.escape() è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦ã€‚
        """
        coll = await self._get_nodes_coll()
        if coll is None:
            return []

        try:
            query_clean = query.strip()[:100]
            if not query_clean:
                return []

            # åˆ†è¯: è‹±æ–‡å•è¯(å«ç‰¹æ®Šå­—ç¬¦å¦‚ C++, .NET) + ä¸­æ–‡ 2-4 å­—è¯ç»„
            tokens = re.findall(r'[a-zA-Z][a-zA-Z0-9+#.]*|[\u4e00-\u9fff]{2,4}', query_clean)
            if not tokens:
                return []

            # å»é‡ + å–å‰ 5 ä¸ª token
            seen = set()
            unique_tokens = []
            for t in tokens:
                t_lower = t.lower()
                if t_lower not in seen:
                    seen.add(t_lower)
                    unique_tokens.append(t)
            unique_tokens = unique_tokens[:5]

            # $or æŸ¥è¯¢: ä»»ä¸€ token åŒ¹é…å³å‘½ä¸­
            regex_filters = [
                {"label": {"$regex": re.escape(t), "$options": "i"}}
                for t in unique_tokens
            ]
            cursor = coll.find(
                {"user_id": user_id, "$or": regex_filters},
                sort=[("mention_count", -1)],
                limit=limit,
            )
            results = []
            async for doc in cursor:
                label = doc.get("label", "")
                if label:
                    results.append(label)
            return results
        except Exception as e:
            logger.debug(f"[Soul] KG find_matching_labels failed: {e}")
            return []

    async def delete_node_cascade(self, user_id: str, node_id: str) -> int:
        """åˆ é™¤èŠ‚ç‚¹ + çº§è”åˆ é™¤æ‰€æœ‰å…³è”è¾¹

        è¿”å›æ€»åˆ é™¤æ•° (èŠ‚ç‚¹ + è¾¹)ã€‚
        """
        nodes_coll = await self._get_nodes_coll()
        edges_coll = await self._get_edges_coll()
        if nodes_coll is None:
            return 0

        total = 0
        try:
            # åˆ é™¤èŠ‚ç‚¹
            result = await nodes_coll.delete_one(
                {"user_id": user_id, "node_id": node_id},
            )
            total += result.deleted_count

            # çº§è”åˆ é™¤å…³è”è¾¹
            if edges_coll is not None:
                result = await edges_coll.delete_many(
                    {"user_id": user_id, "$or": [
                        {"source_id": node_id},
                        {"target_id": node_id},
                    ]},
                )
                total += result.deleted_count
        except Exception as e:
            logger.debug(f"[Soul] KG delete_node_cascade failed: {e}")

        return total
